# TALENT LLM: Multi-Label Talent Prediction Dataset

This repository contains the dataset and code for the paper:

**"TALENT LLM: Multi-Label Talent Prediction in Children Using Fine-Tuned Large Language Models with Calibrated Baselines"**

## Dataset

The dataset contains 5,173 talent analyses from 479 children, generated by a multi-agent LLM system analyzing children's creative artifacts (drawings, writings, audio recordings, videos, etc.).

### Files

```
data/
├── train.jsonl          # Training set (3,947 analyses)
├── val.jsonl            # Validation set (544 analyses)
├── test.jsonl           # Test set (682 analyses)
├── train_temporal.jsonl # Temporal training set (279 children)
└── test_temporal.jsonl  # Temporal test set (70 children)
```

### Data Format

Each line is a JSON object with the following fields:

| Field | Description |
|-------|-------------|
| `child_hash` | Anonymized child identifier (SHA256) |
| `analysis_type` | Artifact type: text, image, musical, audio, video, pdf, etc. |
| `age` | Child's age in years |
| `age_group` | Age band: 5-7, 8-10, 11-13, 14+ |
| `gender` | Gender (male/female/null) |
| `category_scores` | Dict of 306 fine-grained talent scores (0-10) |
| `bin_scores` | Dict of 7 aggregated bin scores |
| `key_talents` | List of top talent categories |
| `text_source_length` | Length of source text (tokens) |
| `split` | Data split (train/val/test) |

### Talent Bins (7 categories)

- **Academic**: verbal_linguistic, logical_mathematical, systematic
- **Sport**: bodily_kinesthetic, physical
- **Art**: spatial_visual, musical_rhythmic, literary
- **Leadership**: interpersonal, strategic
- **Service**: empathy, social
- **Technology**: technical, engineering
- **Others**: naturalistic, intrapersonal

### Artifact Type Distribution

| Type | Count | Percentage |
|------|-------|------------|
| Text | 2,628 | 50.8% |
| Image | 1,562 | 30.2% |
| Musical | 912 | 17.6% |
| Audio | 48 | 0.9% |
| Other | 23 | 0.4% |

## Code

### Notebooks

- `notebooks/arxiv_ml_methods_v3.ipynb` - Complete ML baseline study (Colab-ready)

### Running the Notebook

1. Open in Google Colab
2. Upload data files to `/content/`
3. Run all cells

### Models Implemented

- Logistic Regression (One-vs-Rest)
- LightGBM
- LightGBM with Platt calibration
- Random Forest
- Temporal prediction (S1 → S2)

## Results

See `results/arxiv_ml_results_v3.json` for complete experimental results.

### Key Results

| Model | ROC-AUC | F1-macro | ECE |
|-------|---------|----------|-----|
| LightGBM (per-analysis) | 0.9999 | 0.9972 | 0.0018 |
| LightGBM (child-level) | 0.9911 | 0.9838 | 0.0216 |
| LightGBM (temporal S1→S2) | - | 0.8333 | 0.0693 |

## Figures

- `figures/model_comparison_v3.png` - Model comparison
- `figures/shap_academic_v3.png` - SHAP analysis for Academic bin
- `figures/temporal_comparison_v3.png` - Temporal evaluation
- `figures/analysis_types_v3.png` - Artifact type distribution

## Citation

```bibtex
@article{sergeev2025talent,
  title={TALENT LLM: Multi-Label Talent Prediction in Children Using Fine-Tuned Large Language Models with Calibrated Baselines},
  author={Sergeev, Dmitriy},
  year={2025}
}
```

## License

This dataset is released for research purposes. Please cite the paper if you use this data.

## Contact

- Author: Dmitriy Sergeev
- Organization: TEMNIKOVA LDA
- Email: ntty@me.com
