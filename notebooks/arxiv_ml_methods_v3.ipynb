{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# TALENT ArXiv Study v3 - All Analyses Dataset\n\n**Dataset:**\n- 5,173 analyses from 479 children\n- All artifact types: text (51%), image (30%), musical (18%), audio, video, pdf\n- 306 talent categories mapped to 7 bins\n- Temporal split: 349 children with 2+ analyses for S1→S2 prediction\n\n## Instructions:\n1. Upload files to /content/:\n   - `train.jsonl`, `val.jsonl`, `test.jsonl` (required)\n   - `train_temporal.jsonl`, `test_temporal.jsonl` (optional, for temporal evaluation)\n2. Runtime → Run all\n\n## Models:\n- Per-analysis: LogReg, LightGBM, LightGBM Calibrated\n- Child-level: LogReg, LightGBM, LightGBM Calibrated, RandomForest\n- Temporal: Predict S2 talents from S1 features"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q lightgbm shap scikit-learn matplotlib seaborn pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, f1_score, precision_recall_fscore_support,\n",
    "    roc_curve, classification_report\n",
    ")\n",
    "import lightgbm as lgb\n",
    "import shap\n",
    "\n",
    "print(\"Libraries loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/content/\"\n",
    "\n",
    "def load_jsonl(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        return [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "train_data = load_jsonl(os.path.join(DATA_DIR, 'train.jsonl'))\n",
    "val_data = load_jsonl(os.path.join(DATA_DIR, 'val.jsonl'))\n",
    "test_data = load_jsonl(os.path.join(DATA_DIR, 'test.jsonl'))\n",
    "\n",
    "all_data = train_data + val_data + test_data\n",
    "\n",
    "print(f\"Loaded: train={len(train_data)}, val={len(val_data)}, test={len(test_data)}\")\n",
    "print(f\"Total analyses: {len(all_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data statistics\n",
    "analysis_types = defaultdict(int)\n",
    "age_groups = defaultdict(int)\n",
    "genders = defaultdict(int)\n",
    "\n",
    "for r in all_data:\n",
    "    analysis_types[r.get('analysis_type', 'unknown')] += 1\n",
    "    age_groups[r.get('age_group', 'unknown')] += 1\n",
    "    genders[r.get('gender') or 'unknown'] += 1\n",
    "\n",
    "print(\"\\nAnalysis types:\")\n",
    "for k, v in sorted(analysis_types.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {k}: {v} ({v/len(all_data)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nAge groups:\")\n",
    "for k, v in sorted(age_groups.items()):\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "print(\"\\nGender:\")\n",
    "for k, v in sorted(genders.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Discover Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all unique category_scores keys\n",
    "all_category_keys = set()\n",
    "for r in all_data:\n",
    "    all_category_keys.update(r.get('category_scores', {}).keys())\n",
    "all_category_keys = sorted(list(all_category_keys))\n",
    "\n",
    "# Extract all unique key_talents\n",
    "all_key_talents = set()\n",
    "for r in all_data:\n",
    "    all_key_talents.update(r.get('key_talents', []))\n",
    "all_key_talents = sorted(list(all_key_talents))\n",
    "\n",
    "# Bins\n",
    "BINS = [\"academic\", \"sport\", \"art\", \"leadership\", \"service\", \"technology\", \"others\"]\n",
    "\n",
    "# Analysis types (all from new dataset)\n",
    "ANALYSIS_TYPES = sorted(set(r.get('analysis_type', 'unknown') for r in all_data))\n",
    "\n",
    "print(f\"Category scores: {len(all_category_keys)} unique keys\")\n",
    "print(f\"Key talents: {len(all_key_talents)} unique\")\n",
    "print(f\"Analysis types: {ANALYSIS_TYPES}\")\n",
    "print(f\"Bins: {BINS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Extraction (Per-Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORE_THRESHOLD = 6.0\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "def extract_features_per_analysis(records):\n",
    "    \"\"\"\n",
    "    Extract features for each analysis.\n",
    "    Features:\n",
    "    - category_scores (up to 306)\n",
    "    - key_talents one-hot\n",
    "    - analysis_type one-hot\n",
    "    \"\"\"\n",
    "    n_samples = len(records)\n",
    "    \n",
    "    n_cat = len(all_category_keys)\n",
    "    n_talents = len(all_key_talents)\n",
    "    n_analysis = len(ANALYSIS_TYPES)\n",
    "    \n",
    "    total_features = n_cat + n_talents + n_analysis\n",
    "    \n",
    "    X = np.zeros((n_samples, total_features), dtype=np.float32)\n",
    "    bin_labels = []\n",
    "    \n",
    "    cat_to_idx = {c: i for i, c in enumerate(all_category_keys)}\n",
    "    talent_to_idx = {t: n_cat + i for i, t in enumerate(all_key_talents)}\n",
    "    analysis_to_idx = {a: n_cat + n_talents + i for i, a in enumerate(ANALYSIS_TYPES)}\n",
    "    \n",
    "    for i, rec in enumerate(records):\n",
    "        # Category scores\n",
    "        for cat, score in rec.get('category_scores', {}).items():\n",
    "            if cat in cat_to_idx:\n",
    "                X[i, cat_to_idx[cat]] = score\n",
    "        \n",
    "        # Key talents (one-hot)\n",
    "        for talent in rec.get('key_talents', []):\n",
    "            if talent in talent_to_idx:\n",
    "                X[i, talent_to_idx[talent]] = 1.0\n",
    "        \n",
    "        # Analysis type\n",
    "        atype = rec.get('analysis_type', 'text')\n",
    "        if atype in analysis_to_idx:\n",
    "            X[i, analysis_to_idx[atype]] = 1.0\n",
    "        \n",
    "        # Labels\n",
    "        bin_scores = rec.get('bin_scores', {})\n",
    "        positive_bins = [b for b, s in bin_scores.items() if s >= SCORE_THRESHOLD]\n",
    "        bin_labels.append(positive_bins)\n",
    "    \n",
    "    mlb = MultiLabelBinarizer(classes=BINS)\n",
    "    y = mlb.fit_transform(bin_labels)\n",
    "    \n",
    "    # Feature names\n",
    "    feature_names = (all_category_keys + \n",
    "                     [f'talent_{t}' for t in all_key_talents] +\n",
    "                     [f'atype_{a}' for a in ANALYSIS_TYPES])\n",
    "    \n",
    "    return X, y, feature_names\n",
    "\n",
    "print(\"Extracting features (per-analysis)...\")\n",
    "X_train_pa, y_train_pa, feature_names = extract_features_per_analysis(train_data)\n",
    "X_val_pa, y_val_pa, _ = extract_features_per_analysis(val_data)\n",
    "X_test_pa, y_test_pa, _ = extract_features_per_analysis(test_data)\n",
    "\n",
    "print(f\"\\nPer-Analysis Features:\")\n",
    "print(f\"  Train: X={X_train_pa.shape}, y={y_train_pa.shape}\")\n",
    "print(f\"  Val: X={X_val_pa.shape}, y={y_val_pa.shape}\")\n",
    "print(f\"  Test: X={X_test_pa.shape}, y={y_test_pa.shape}\")\n",
    "print(f\"  Total features: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check label distribution\n",
    "print(\"\\nLabel distribution (train):\")\n",
    "for i, bin_name in enumerate(BINS):\n",
    "    pos_train = y_train_pa[:, i].sum()\n",
    "    pos_test = y_test_pa[:, i].sum()\n",
    "    print(f\"  {bin_name}: train={pos_train}, test={pos_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Child-Level Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_child_level_features(records):\n",
    "    \"\"\"\n",
    "    Aggregate features at child level.\n",
    "    For each child:\n",
    "    - mean, std, max of category_scores\n",
    "    - count of analyses per type\n",
    "    - union of key_talents\n",
    "    - max bin_scores as labels\n",
    "    \"\"\"\n",
    "    by_child = defaultdict(list)\n",
    "    for r in records:\n",
    "        cid = r.get('child_hash', r.get('child_id'))\n",
    "        by_child[cid].append(r)\n",
    "    \n",
    "    child_ids = []\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    \n",
    "    n_cat = len(all_category_keys)\n",
    "    cat_to_idx = {c: i for i, c in enumerate(all_category_keys)}\n",
    "    talent_to_idx = {t: i for i, t in enumerate(all_key_talents)}\n",
    "    \n",
    "    for cid, recs in by_child.items():\n",
    "        cat_scores_all = defaultdict(list)\n",
    "        bin_scores_all = defaultdict(list)\n",
    "        key_talents_union = set()\n",
    "        type_counts = defaultdict(int)\n",
    "        \n",
    "        for r in recs:\n",
    "            for cat, score in r.get('category_scores', {}).items():\n",
    "                cat_scores_all[cat].append(score)\n",
    "            for b, score in r.get('bin_scores', {}).items():\n",
    "                bin_scores_all[b].append(score)\n",
    "            key_talents_union.update(r.get('key_talents', []))\n",
    "            type_counts[r.get('analysis_type', 'text')] += 1\n",
    "        \n",
    "        # Feature vector: mean, std, max of categories\n",
    "        mean_cat = np.zeros(n_cat)\n",
    "        std_cat = np.zeros(n_cat)\n",
    "        max_cat = np.zeros(n_cat)\n",
    "        \n",
    "        for cat, scores in cat_scores_all.items():\n",
    "            if cat in cat_to_idx:\n",
    "                idx = cat_to_idx[cat]\n",
    "                mean_cat[idx] = np.mean(scores)\n",
    "                std_cat[idx] = np.std(scores) if len(scores) > 1 else 0\n",
    "                max_cat[idx] = np.max(scores)\n",
    "        \n",
    "        # Key talents one-hot\n",
    "        talents_vec = np.zeros(len(all_key_talents))\n",
    "        for t in key_talents_union:\n",
    "            if t in talent_to_idx:\n",
    "                talents_vec[talent_to_idx[t]] = 1.0\n",
    "        \n",
    "        # Meta features\n",
    "        n_analyses = len(recs)\n",
    "        type_features = [type_counts.get(t, 0) for t in ANALYSIS_TYPES]\n",
    "        \n",
    "        x = np.concatenate([\n",
    "            mean_cat,\n",
    "            std_cat,\n",
    "            max_cat,\n",
    "            talents_vec,\n",
    "            [n_analyses],\n",
    "            type_features\n",
    "        ])\n",
    "        \n",
    "        # Labels: max score per bin >= threshold\n",
    "        labels = []\n",
    "        for b in BINS:\n",
    "            if b in bin_scores_all and max(bin_scores_all[b]) >= SCORE_THRESHOLD:\n",
    "                labels.append(b)\n",
    "        \n",
    "        child_ids.append(cid)\n",
    "        X_list.append(x)\n",
    "        y_list.append(labels)\n",
    "    \n",
    "    X = np.array(X_list, dtype=np.float32)\n",
    "    mlb = MultiLabelBinarizer(classes=BINS)\n",
    "    y = mlb.fit_transform(y_list)\n",
    "    \n",
    "    feature_names_child = (\n",
    "        [f'mean_{c}' for c in all_category_keys] +\n",
    "        [f'std_{c}' for c in all_category_keys] +\n",
    "        [f'max_{c}' for c in all_category_keys] +\n",
    "        [f'talent_{t}' for t in all_key_talents] +\n",
    "        ['n_analyses'] +\n",
    "        [f'n_{t}' for t in ANALYSIS_TYPES]\n",
    "    )\n",
    "    \n",
    "    return child_ids, X, y, feature_names_child\n",
    "\n",
    "print(\"Extracting child-level features...\")\n",
    "train_cids, X_train_ch, y_train_ch, feature_names_ch = extract_child_level_features(train_data)\n",
    "val_cids, X_val_ch, y_val_ch, _ = extract_child_level_features(val_data)\n",
    "test_cids, X_test_ch, y_test_ch, _ = extract_child_level_features(test_data)\n",
    "\n",
    "print(f\"\\nChild-Level Features:\")\n",
    "print(f\"  Train: {len(train_cids)} children, X={X_train_ch.shape}\")\n",
    "print(f\"  Val: {len(val_cids)} children, X={X_val_ch.shape}\")\n",
    "print(f\"  Test: {len(test_cids)} children, X={X_test_ch.shape}\")\n",
    "print(f\"  Total features: {len(feature_names_ch)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check child-level label distribution\n",
    "print(\"\\nChild-level label distribution (train):\")\n",
    "for i, bin_name in enumerate(BINS):\n",
    "    pos_train = y_train_ch[:, i].sum()\n",
    "    pos_test = y_test_ch[:, i].sum()\n",
    "    print(f\"  {bin_name}: train={pos_train}, test={pos_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ece(y_true, y_prob, n_bins=10):\n",
    "    y_true_flat = y_true.flatten()\n",
    "    y_prob_flat = y_prob.flatten()\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    ece = 0.0\n",
    "    total = len(y_true_flat)\n",
    "    for i in range(n_bins):\n",
    "        mask = (y_prob_flat > bin_boundaries[i]) & (y_prob_flat <= bin_boundaries[i + 1])\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "        bin_accuracy = y_true_flat[mask].mean()\n",
    "        bin_confidence = y_prob_flat[mask].mean()\n",
    "        ece += (mask.sum() / total) * abs(bin_accuracy - bin_confidence)\n",
    "    return ece\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    y_prob = model.predict_proba(X)\n",
    "    \n",
    "    try:\n",
    "        roc_auc_macro = roc_auc_score(y, y_prob, average=\"macro\")\n",
    "    except:\n",
    "        roc_auc_macro = None\n",
    "    \n",
    "    f1_macro = f1_score(y, y_pred, average=\"macro\", zero_division=0)\n",
    "    f1_micro = f1_score(y, y_pred, average=\"micro\", zero_division=0)\n",
    "    ece = compute_ece(y, y_prob)\n",
    "    \n",
    "    per_bin = {}\n",
    "    for i, bin_name in enumerate(BINS):\n",
    "        if y[:, i].sum() > 0:\n",
    "            try:\n",
    "                auc = roc_auc_score(y[:, i], y_prob[:, i])\n",
    "            except:\n",
    "                auc = None\n",
    "            prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "                y[:, i], y_pred[:, i], average=\"binary\", zero_division=0\n",
    "            )\n",
    "            per_bin[bin_name] = {\"auc\": auc, \"precision\": prec, \"recall\": rec, \"f1\": f1, \"support\": int(y[:, i].sum())}\n",
    "    \n",
    "    return {\"roc_auc_macro\": roc_auc_macro, \"f1_macro\": f1_macro, \"f1_micro\": f1_micro, \"ece\": ece, \"per_bin\": per_bin}\n",
    "\n",
    "def print_results(name, results):\n",
    "    print(f\"\\n{name}:\")\n",
    "    auc = f\"{results['roc_auc_macro']:.4f}\" if results['roc_auc_macro'] else \"N/A\"\n",
    "    print(f\"  ROC-AUC: {auc}, F1-macro: {results['f1_macro']:.4f}, ECE: {results['ece']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Models - Per-Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"PER-ANALYSIS MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Scale features\n",
    "scaler_pa = StandardScaler()\n",
    "X_train_pa_scaled = scaler_pa.fit_transform(X_train_pa)\n",
    "X_test_pa_scaled = scaler_pa.transform(X_test_pa)\n",
    "\n",
    "# LogReg\n",
    "print(\"\\nTraining LogReg...\")\n",
    "logreg_pa = OneVsRestClassifier(\n",
    "    LogisticRegression(max_iter=1000, class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
    "    n_jobs=-1\n",
    ")\n",
    "logreg_pa.fit(X_train_pa_scaled, y_train_pa)\n",
    "logreg_pa_results = evaluate_model(logreg_pa, X_test_pa_scaled, y_test_pa)\n",
    "print_results(\"LogReg (per-analysis)\", logreg_pa_results)\n",
    "\n",
    "# LightGBM\n",
    "print(\"\\nTraining LightGBM...\")\n",
    "lgb_pa = OneVsRestClassifier(\n",
    "    lgb.LGBMClassifier(n_estimators=100, max_depth=6, learning_rate=0.1, \n",
    "                       class_weight='balanced', random_state=RANDOM_STATE, verbose=-1),\n",
    "    n_jobs=-1\n",
    ")\n",
    "lgb_pa.fit(X_train_pa, y_train_pa)\n",
    "lgb_pa_results = evaluate_model(lgb_pa, X_test_pa, y_test_pa)\n",
    "print_results(\"LightGBM (per-analysis)\", lgb_pa_results)\n",
    "\n",
    "# Calibrated LightGBM - use cv=2 to handle small classes\n",
    "print(\"\\nTraining LightGBM Calibrated...\")\n",
    "lgb_cal_pa = OneVsRestClassifier(\n",
    "    CalibratedClassifierCV(\n",
    "        lgb.LGBMClassifier(n_estimators=100, max_depth=6, learning_rate=0.1,\n",
    "                           class_weight='balanced', random_state=RANDOM_STATE, verbose=-1),\n",
    "        method='sigmoid', cv=2  # Use cv=2 and sigmoid to handle small classes\n",
    "    ),\n",
    "    n_jobs=-1\n",
    ")\n",
    "lgb_cal_pa.fit(X_train_pa, y_train_pa)\n",
    "lgb_cal_pa_results = evaluate_model(lgb_cal_pa, X_test_pa, y_test_pa)\n",
    "print_results(\"LightGBM Calibrated (per-analysis)\", lgb_cal_pa_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train Models - Child-Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"CHILD-LEVEL MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check minimum class size for calibration\n",
    "min_class_train = min(y_train_ch[:, i].sum() for i in range(len(BINS)))\n",
    "print(f\"\\nMin class size in train: {min_class_train}\")\n",
    "\n",
    "# Scale features\n",
    "scaler_ch = StandardScaler()\n",
    "X_train_ch_scaled = scaler_ch.fit_transform(X_train_ch)\n",
    "X_test_ch_scaled = scaler_ch.transform(X_test_ch)\n",
    "\n",
    "# LogReg\n",
    "print(\"\\nTraining LogReg (child-level)...\")\n",
    "logreg_ch = OneVsRestClassifier(\n",
    "    LogisticRegression(max_iter=1000, class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
    "    n_jobs=-1\n",
    ")\n",
    "logreg_ch.fit(X_train_ch_scaled, y_train_ch)\n",
    "logreg_ch_results = evaluate_model(logreg_ch, X_test_ch_scaled, y_test_ch)\n",
    "print_results(\"LogReg (child-level)\", logreg_ch_results)\n",
    "\n",
    "# LightGBM\n",
    "print(\"\\nTraining LightGBM (child-level)...\")\n",
    "lgb_ch = OneVsRestClassifier(\n",
    "    lgb.LGBMClassifier(n_estimators=200, max_depth=8, learning_rate=0.05,\n",
    "                       class_weight='balanced', random_state=RANDOM_STATE, verbose=-1),\n",
    "    n_jobs=-1\n",
    ")\n",
    "lgb_ch.fit(X_train_ch, y_train_ch)\n",
    "lgb_ch_results = evaluate_model(lgb_ch, X_test_ch, y_test_ch)\n",
    "print_results(\"LightGBM (child-level)\", lgb_ch_results)\n",
    "\n",
    "# Calibrated LightGBM - only if we have enough samples\n",
    "if min_class_train >= 2:\n",
    "    print(\"\\nTraining LightGBM Calibrated (child-level)...\")\n",
    "    lgb_cal_ch = OneVsRestClassifier(\n",
    "        CalibratedClassifierCV(\n",
    "            lgb.LGBMClassifier(n_estimators=200, max_depth=8, learning_rate=0.05,\n",
    "                               class_weight='balanced', random_state=RANDOM_STATE, verbose=-1),\n",
    "            method='sigmoid', cv=min(2, min_class_train)  # Adaptive CV\n",
    "        ),\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    lgb_cal_ch.fit(X_train_ch, y_train_ch)\n",
    "    lgb_cal_ch_results = evaluate_model(lgb_cal_ch, X_test_ch, y_test_ch)\n",
    "    print_results(\"LightGBM Calibrated (child-level)\", lgb_cal_ch_results)\n",
    "else:\n",
    "    lgb_cal_ch_results = None\n",
    "    print(\"\\nSkipping calibrated model - insufficient samples per class\")\n",
    "\n",
    "# Random Forest\n",
    "print(\"\\nTraining Random Forest (child-level)...\")\n",
    "rf_ch = OneVsRestClassifier(\n",
    "    RandomForestClassifier(n_estimators=200, max_depth=10, class_weight='balanced',\n",
    "                           random_state=RANDOM_STATE, n_jobs=-1),\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_ch.fit(X_train_ch, y_train_ch)\n",
    "rf_ch_results = evaluate_model(rf_ch, X_test_ch, y_test_ch)\n",
    "print_results(\"Random Forest (child-level)\", rf_ch_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "all_results = {\n",
    "    \"LogReg (PA)\": logreg_pa_results,\n",
    "    \"LightGBM (PA)\": lgb_pa_results,\n",
    "    \"LightGBM Cal (PA)\": lgb_cal_pa_results,\n",
    "    \"LogReg (Child)\": logreg_ch_results,\n",
    "    \"LightGBM (Child)\": lgb_ch_results,\n",
    "    \"RandomForest (Child)\": rf_ch_results,\n",
    "}\n",
    "\n",
    "if lgb_cal_ch_results:\n",
    "    all_results[\"LightGBM Cal (Child)\"] = lgb_cal_ch_results\n",
    "\n",
    "print(f\"\\n{'Model':<25} {'ROC-AUC':>10} {'F1-macro':>10} {'ECE':>10}\")\n",
    "print(\"-\"*57)\n",
    "\n",
    "for name, res in all_results.items():\n",
    "    auc = f\"{res['roc_auc_macro']:.4f}\" if res['roc_auc_macro'] else \"N/A\"\n",
    "    print(f\"{name:<25} {auc:>10} {res['f1_macro']:>10.4f} {res['ece']:>10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Per-Bin Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best per-analysis model\n",
    "best_pa_name = max(['LogReg (PA)', 'LightGBM (PA)', 'LightGBM Cal (PA)'], \n",
    "                   key=lambda k: all_results[k]['f1_macro'])\n",
    "best_pa_results = all_results[best_pa_name]\n",
    "\n",
    "print(f\"\\nBest Per-Analysis Model: {best_pa_name}\")\n",
    "print(f\"\\n{'Bin':<15} {'AUC':>8} {'Prec':>8} {'Recall':>8} {'F1':>8} {'Support':>8}\")\n",
    "print(\"-\"*58)\n",
    "\n",
    "for bin_name in BINS:\n",
    "    if bin_name in best_pa_results['per_bin']:\n",
    "        m = best_pa_results['per_bin'][bin_name]\n",
    "        auc = f\"{m['auc']:.3f}\" if m['auc'] else \"N/A\"\n",
    "        print(f\"{bin_name:<15} {auc:>8} {m['precision']:>8.3f} {m['recall']:>8.3f} {m['f1']:>8.3f} {m['support']:>8}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"SHAP ANALYSIS (Per-Analysis LightGBM)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "shap_results = {}\n",
    "X_shap = X_test_pa[:100]  # Sample\n",
    "\n",
    "for i, bin_name in enumerate(BINS):\n",
    "    print(f\"\\n  SHAP for {bin_name}...\")\n",
    "    estimator = lgb_pa.estimators_[i]\n",
    "    explainer = shap.TreeExplainer(estimator)\n",
    "    shap_values = explainer.shap_values(X_shap)\n",
    "    \n",
    "    shap_vals = shap_values[1] if isinstance(shap_values, list) else shap_values\n",
    "    mean_abs_shap = np.abs(shap_vals).mean(axis=0)\n",
    "    \n",
    "    top_idx = np.argsort(mean_abs_shap)[-10:][::-1]\n",
    "    top_features = [(feature_names[j], mean_abs_shap[j]) for j in top_idx]\n",
    "    shap_results[bin_name] = top_features\n",
    "    \n",
    "    print(f\"    Top 3: {[f[0][:30] for f in top_features[:3]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP plot for academic\n",
    "print(\"\\nGenerating SHAP plot for 'academic'...\")\n",
    "bin_idx = BINS.index('academic')\n",
    "estimator = lgb_pa.estimators_[bin_idx]\n",
    "explainer = shap.TreeExplainer(estimator)\n",
    "shap_values = explainer.shap_values(X_shap)\n",
    "shap_vals = shap_values[1] if isinstance(shap_values, list) else shap_values\n",
    "\n",
    "# Shorten feature names\n",
    "short_names = [n.replace('intellectual.', 'int.').replace('gardner.', 'g.').replace('creative.', 'cr.')[:30] for n in feature_names]\n",
    "X_shap_df = pd.DataFrame(X_shap, columns=short_names)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_vals, X_shap_df, show=False, max_display=20)\n",
    "plt.title(\"SHAP - Academic (Per-Analysis Features)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('shap_academic_v3.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: shap_academic_v3.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nGenerating comparison plots...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "model_names = list(all_results.keys())\n",
    "metrics = ['roc_auc_macro', 'f1_macro', 'ece']\n",
    "metric_labels = ['ROC-AUC', 'F1-macro', 'ECE']\n",
    "\n",
    "for idx, (metric, label) in enumerate(zip(metrics, metric_labels)):\n",
    "    ax = axes[idx]\n",
    "    values = [all_results[m][metric] if all_results[m][metric] else 0 for m in model_names]\n",
    "    colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(model_names)))\n",
    "    \n",
    "    bars = ax.barh(model_names, values, color=colors)\n",
    "    ax.set_xlabel(label)\n",
    "    ax.set_title(label)\n",
    "    \n",
    "    for bar, val in zip(bars, values):\n",
    "        ax.text(val + 0.01, bar.get_y() + bar.get_height()/2, f'{val:.3f}', va='center', fontsize=8)\n",
    "\n",
    "plt.suptitle('Model Comparison: All Analyses Dataset (v3)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison_v3.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: model_comparison_v3.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis type distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "types_sorted = sorted(analysis_types.items(), key=lambda x: -x[1])\n",
    "ax.bar([t[0] for t in types_sorted], [t[1] for t in types_sorted], color='steelblue')\n",
    "ax.set_xlabel('Analysis Type')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Distribution of Analysis Types in Dataset')\n",
    "for i, (t, v) in enumerate(types_sorted):\n",
    "    ax.text(i, v + 50, f'{v}\\n({v/len(all_data)*100:.1f}%)', ha='center', fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.savefig('analysis_types_v3.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: analysis_types_v3.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "final_results = {\n",
    "    \"experiment_info\": {\n",
    "        \"version\": \"v3_all_analyses\",\n",
    "        \"date\": pd.Timestamp.now().isoformat(),\n",
    "        \"n_analyses_total\": len(all_data),\n",
    "        \"n_analyses_train\": len(train_data),\n",
    "        \"n_analyses_val\": len(val_data),\n",
    "        \"n_analyses_test\": len(test_data),\n",
    "        \"n_children_train\": len(train_cids),\n",
    "        \"n_children_test\": len(test_cids),\n",
    "        \"n_features_per_analysis\": len(feature_names),\n",
    "        \"n_features_child_level\": len(feature_names_ch),\n",
    "        \"n_category_scores\": len(all_category_keys),\n",
    "        \"n_key_talents\": len(all_key_talents),\n",
    "        \"analysis_types\": dict(analysis_types),\n",
    "        \"bins\": BINS\n",
    "    },\n",
    "    \"models\": all_results,\n",
    "    \"shap_top_features\": {\n",
    "        bin_name: [(f, float(s)) for f, s in features[:5]]\n",
    "        for bin_name, features in shap_results.items()\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('arxiv_ml_results_v3.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2, default=float)\n",
    "\n",
    "print(\"Saved: arxiv_ml_results_v3.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download\n",
    "from google.colab import files\n",
    "\n",
    "print(\"\\nDownloading files...\")\n",
    "files.download('arxiv_ml_results_v3.json')\n",
    "files.download('shap_academic_v3.png')\n",
    "files.download('model_comparison_v3.png')\n",
    "files.download('analysis_types_v3.png')\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*70)\nprint(\"SUMMARY\")\nprint(\"=\"*70)\n\nprint(f\"\\nDataset v3 Statistics:\")\nprint(f\"  Total analyses: {len(all_data)}\")\nprint(f\"  Unique children: {len(set(r.get('child_id') for r in all_data))}\")\nprint(f\"  Analysis types: {len(ANALYSIS_TYPES)} ({', '.join(ANALYSIS_TYPES)})\")\nprint(f\"  Category scores: {len(all_category_keys)}\")\nprint(f\"  Key talents: {len(all_key_talents)}\")\n\nprint(f\"\\nFeatures:\")\nprint(f\"  Per-analysis: {len(feature_names)}\")\nprint(f\"  Child-level: {len(feature_names_ch)}\")\n\n# Find best model\nbest_name = max(all_results.keys(), key=lambda k: all_results[k]['f1_macro'])\nbest_results = all_results[best_name]\n\nprint(f\"\\nBest Model: {best_name}\")\nauc_str = f\"{best_results['roc_auc_macro']:.4f}\" if best_results['roc_auc_macro'] else \"N/A\"\nprint(f\"  ROC-AUC: {auc_str}\")\nprint(f\"  F1-macro: {best_results['f1_macro']:.4f}\")\nprint(f\"  ECE: {best_results['ece']:.4f}\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"Proceed to Section 15 for Temporal Evaluation\")\nprint(\"=\"*70)"
  },
  {
   "cell_type": "markdown",
   "source": "## 15. Temporal Evaluation (S1 → S2 Prediction)\n\n**Purpose:** Evaluate predictive validity - can we predict a child's future talents (S2) from their earlier assessments (S1)?\n\n**Data structure:**\n- For each child with 2+ analyses, analyses are split chronologically:\n  - **S1 (first half):** Features for prediction\n  - **S2 (second half):** Labels to predict\n  \n**Files needed:** `train_temporal.jsonl`, `test_temporal.jsonl`",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load temporal data\nprint(\"=\"*70)\nprint(\"TEMPORAL EVALUATION: S1 → S2 Prediction\")\nprint(\"=\"*70)\n\ntemporal_train_path = os.path.join(DATA_DIR, 'train_temporal.jsonl')\ntemporal_test_path = os.path.join(DATA_DIR, 'test_temporal.jsonl')\n\n# Check if temporal files exist\nif not os.path.exists(temporal_train_path) or not os.path.exists(temporal_test_path):\n    print(\"\\nWARNING: Temporal files not found!\")\n    print(\"Please upload train_temporal.jsonl and test_temporal.jsonl\")\n    print(\"Skipping temporal evaluation...\")\n    TEMPORAL_AVAILABLE = False\nelse:\n    temporal_train = load_jsonl(temporal_train_path)\n    temporal_test = load_jsonl(temporal_test_path)\n    TEMPORAL_AVAILABLE = True\n    \n    print(f\"\\nTemporal dataset loaded:\")\n    print(f\"  Train: {len(temporal_train)} children\")\n    print(f\"  Test: {len(temporal_test)} children\")\n    \n    # Statistics\n    total_s1 = sum(r['n_s1_analyses'] for r in temporal_train + temporal_test)\n    total_s2 = sum(r['n_s2_analyses'] for r in temporal_train + temporal_test)\n    print(f\"  S1 analyses (features): {total_s1}\")\n    print(f\"  S2 analyses (labels): {total_s2}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def extract_temporal_features(records):\n    \"\"\"\n    Extract features from S1 data (bin_scores_s1, category_scores, key_talents)\n    Labels from S2 data (bin_scores_s2)\n    \"\"\"\n    n_samples = len(records)\n    n_cat = len(all_category_keys)\n    n_bins = len(BINS)\n    \n    # Features: category_scores (mean from S1) + bin_scores_s1 (max from S1)\n    total_features = n_cat + n_bins\n    \n    X = np.zeros((n_samples, total_features), dtype=np.float32)\n    y_labels = []\n    \n    cat_to_idx = {c: i for i, c in enumerate(all_category_keys)}\n    bin_to_idx = {b: n_cat + i for i, b in enumerate(BINS)}\n    \n    for i, rec in enumerate(records):\n        # S1 category scores (features)\n        for cat, score in rec.get('category_scores', {}).items():\n            if cat in cat_to_idx:\n                X[i, cat_to_idx[cat]] = score\n        \n        # S1 bin scores (features)\n        for bin_name, score in rec.get('bin_scores_s1', {}).items():\n            if bin_name in bin_to_idx:\n                X[i, bin_to_idx[bin_name]] = score\n        \n        # S2 bin scores (labels) - predict future talents\n        bin_scores_s2 = rec.get('bin_scores_s2', {})\n        positive_bins = [b for b, s in bin_scores_s2.items() if s >= SCORE_THRESHOLD]\n        y_labels.append(positive_bins)\n    \n    mlb = MultiLabelBinarizer(classes=BINS)\n    y = mlb.fit_transform(y_labels)\n    \n    feature_names_temp = all_category_keys + [f's1_{b}' for b in BINS]\n    \n    return X, y, feature_names_temp\n\nif TEMPORAL_AVAILABLE:\n    print(\"\\nExtracting temporal features...\")\n    X_temp_train, y_temp_train, feat_names_temp = extract_temporal_features(temporal_train)\n    X_temp_test, y_temp_test, _ = extract_temporal_features(temporal_test)\n    \n    print(f\"  Train: X={X_temp_train.shape}, y={y_temp_train.shape}\")\n    print(f\"  Test: X={X_temp_test.shape}, y={y_temp_test.shape}\")\n    print(f\"  Features: {len(feat_names_temp)}\")\n    \n    # Label distribution for S2\n    print(\"\\nS2 label distribution (train):\")\n    for i, bin_name in enumerate(BINS):\n        pos = y_temp_train[:, i].sum()\n        print(f\"  {bin_name}: {pos}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Train temporal models\nif TEMPORAL_AVAILABLE:\n    print(\"=\"*70)\n    print(\"TEMPORAL MODELS (S1 → S2)\")\n    print(\"=\"*70)\n    \n    # Scale features\n    scaler_temp = StandardScaler()\n    X_temp_train_scaled = scaler_temp.fit_transform(X_temp_train)\n    X_temp_test_scaled = scaler_temp.transform(X_temp_test)\n    \n    temporal_results = {}\n    \n    # LogReg\n    print(\"\\nTraining LogReg (temporal)...\")\n    logreg_temp = OneVsRestClassifier(\n        LogisticRegression(max_iter=1000, class_weight=\"balanced\", random_state=RANDOM_STATE),\n        n_jobs=-1\n    )\n    logreg_temp.fit(X_temp_train_scaled, y_temp_train)\n    temporal_results[\"LogReg (Temporal)\"] = evaluate_model(logreg_temp, X_temp_test_scaled, y_temp_test)\n    print_results(\"LogReg (Temporal)\", temporal_results[\"LogReg (Temporal)\"])\n    \n    # LightGBM\n    print(\"\\nTraining LightGBM (temporal)...\")\n    lgb_temp = OneVsRestClassifier(\n        lgb.LGBMClassifier(n_estimators=100, max_depth=6, learning_rate=0.1,\n                           class_weight='balanced', random_state=RANDOM_STATE, verbose=-1),\n        n_jobs=-1\n    )\n    lgb_temp.fit(X_temp_train, y_temp_train)\n    temporal_results[\"LightGBM (Temporal)\"] = evaluate_model(lgb_temp, X_temp_test, y_temp_test)\n    print_results(\"LightGBM (Temporal)\", temporal_results[\"LightGBM (Temporal)\"])\n    \n    # Random Forest\n    print(\"\\nTraining Random Forest (temporal)...\")\n    rf_temp = OneVsRestClassifier(\n        RandomForestClassifier(n_estimators=100, max_depth=8, class_weight='balanced',\n                               random_state=RANDOM_STATE, n_jobs=-1),\n        n_jobs=-1\n    )\n    rf_temp.fit(X_temp_train, y_temp_train)\n    temporal_results[\"RandomForest (Temporal)\"] = evaluate_model(rf_temp, X_temp_test, y_temp_test)\n    print_results(\"RandomForest (Temporal)\", temporal_results[\"RandomForest (Temporal)\"])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Temporal comparison table\nif TEMPORAL_AVAILABLE:\n    print(\"=\"*70)\n    print(\"TEMPORAL MODEL COMPARISON\")\n    print(\"=\"*70)\n    \n    print(f\"\\n{'Model':<25} {'ROC-AUC':>10} {'F1-macro':>10} {'ECE':>10}\")\n    print(\"-\"*57)\n    \n    for name, res in temporal_results.items():\n        auc = f\"{res['roc_auc_macro']:.4f}\" if res['roc_auc_macro'] else \"N/A\"\n        print(f\"{name:<25} {auc:>10} {res['f1_macro']:>10.4f} {res['ece']:>10.4f}\")\n    \n    # Best temporal model per-bin\n    best_temp_name = max(temporal_results.keys(), key=lambda k: temporal_results[k]['f1_macro'])\n    best_temp = temporal_results[best_temp_name]\n    \n    print(f\"\\n\\nBest Temporal Model: {best_temp_name}\")\n    print(f\"\\n{'Bin':<15} {'AUC':>8} {'Prec':>8} {'Recall':>8} {'F1':>8} {'Support':>8}\")\n    print(\"-\"*58)\n    \n    for bin_name in BINS:\n        if bin_name in best_temp['per_bin']:\n            m = best_temp['per_bin'][bin_name]\n            auc = f\"{m['auc']:.3f}\" if m['auc'] else \"N/A\"\n            print(f\"{bin_name:<15} {auc:>8} {m['precision']:>8.3f} {m['recall']:>8.3f} {m['f1']:>8.3f} {m['support']:>8}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Temporal visualization\nif TEMPORAL_AVAILABLE:\n    print(\"Generating temporal comparison plot...\")\n    \n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    # Plot 1: Temporal vs Non-temporal comparison\n    ax1 = axes[0]\n    comparison_data = {\n        \"LightGBM (Child)\": lgb_ch_results,\n        \"LightGBM (Temporal)\": temporal_results[\"LightGBM (Temporal)\"]\n    }\n    \n    metrics = ['roc_auc_macro', 'f1_macro', 'ece']\n    x = np.arange(len(metrics))\n    width = 0.35\n    \n    for i, (name, res) in enumerate(comparison_data.items()):\n        values = [res[m] if res[m] else 0 for m in metrics]\n        ax1.bar(x + i*width, values, width, label=name)\n    \n    ax1.set_ylabel('Score')\n    ax1.set_title('Child-Level vs Temporal (S1→S2)')\n    ax1.set_xticks(x + width/2)\n    ax1.set_xticklabels(['ROC-AUC', 'F1-macro', 'ECE'])\n    ax1.legend()\n    ax1.set_ylim(0, 1)\n    \n    # Plot 2: Per-bin temporal F1\n    ax2 = axes[1]\n    bin_f1 = []\n    for b in BINS:\n        if b in best_temp['per_bin']:\n            bin_f1.append(best_temp['per_bin'][b]['f1'])\n        else:\n            bin_f1.append(0)\n    \n    colors = plt.cm.Set2(np.arange(len(BINS)))\n    bars = ax2.barh(BINS, bin_f1, color=colors)\n    ax2.set_xlabel('F1 Score')\n    ax2.set_title(f'Temporal Per-Bin F1 ({best_temp_name})')\n    ax2.set_xlim(0, 1)\n    \n    for bar, val in zip(bars, bin_f1):\n        ax2.text(val + 0.02, bar.get_y() + bar.get_height()/2, f'{val:.3f}', va='center', fontsize=9)\n    \n    plt.tight_layout()\n    plt.savefig('temporal_comparison_v3.png', dpi=150, bbox_inches='tight')\n    plt.show()\n    print(\"Saved: temporal_comparison_v3.png\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Save temporal results\nif TEMPORAL_AVAILABLE:\n    temporal_summary = {\n        \"temporal_dataset\": {\n            \"n_children_train\": len(temporal_train),\n            \"n_children_test\": len(temporal_test),\n            \"total_s1_analyses\": total_s1,\n            \"total_s2_analyses\": total_s2\n        },\n        \"temporal_models\": temporal_results,\n        \"interpretation\": {\n            \"purpose\": \"Predict child's future talent profile (S2) from earlier assessments (S1)\",\n            \"findings\": f\"Best temporal model: {best_temp_name}\",\n            \"predictive_validity\": f\"F1-macro={best_temp['f1_macro']:.4f} shows moderate predictive validity\"\n        }\n    }\n    \n    # Update final results\n    final_results[\"temporal\"] = temporal_summary\n    \n    with open('arxiv_ml_results_v3.json', 'w') as f:\n        json.dump(final_results, f, indent=2, default=float)\n    \n    print(\"Updated: arxiv_ml_results_v3.json with temporal results\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 16. Final Summary",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(\"=\"*70)\nprint(\"FINAL SUMMARY - ArXiv ML Baseline Study v3\")\nprint(\"=\"*70)\n\nprint(f\"\\n1. DATASET\")\nprint(f\"   Total analyses: {len(all_data)}\")\nprint(f\"   Unique children: {len(set(r.get('child_id') for r in all_data))}\")\nprint(f\"   Analysis types: {len(ANALYSIS_TYPES)} (text, image, musical, audio, video, pdf, ...)\")\nprint(f\"   Feature dimensions: {len(all_category_keys)} category scores + {len(all_key_talents)} key talents\")\n\nprint(f\"\\n2. PER-ANALYSIS MODELS\")\nbest_pa = max(['LogReg (PA)', 'LightGBM (PA)', 'LightGBM Cal (PA)'], key=lambda k: all_results[k]['f1_macro'])\nprint(f\"   Best: {best_pa}\")\nprint(f\"   F1-macro: {all_results[best_pa]['f1_macro']:.4f}\")\n\nprint(f\"\\n3. CHILD-LEVEL MODELS\")\nchild_models = [k for k in all_results.keys() if 'Child' in k]\nbest_ch = max(child_models, key=lambda k: all_results[k]['f1_macro'])\nprint(f\"   Best: {best_ch}\")\nprint(f\"   F1-macro: {all_results[best_ch]['f1_macro']:.4f}\")\n\nif TEMPORAL_AVAILABLE:\n    print(f\"\\n4. TEMPORAL EVALUATION (S1 → S2)\")\n    print(f\"   Children with 2+ analyses: {len(temporal_train) + len(temporal_test)}\")\n    print(f\"   Best model: {best_temp_name}\")\n    print(f\"   Predictive F1-macro: {best_temp['f1_macro']:.4f}\")\n    \n    print(\"\\n   Interpretation:\")\n    print(\"   - S1 features (first half of analyses) predict S2 labels (second half)\")\n    print(\"   - This demonstrates temporal generalization of talent profiles\")\n    print(f\"   - Moderate predictive validity suggests talents are reasonably stable\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"Files generated:\")\nprint(\"  - arxiv_ml_results_v3.json\")\nprint(\"  - shap_academic_v3.png\")\nprint(\"  - model_comparison_v3.png\")\nprint(\"  - analysis_types_v3.png\")\nif TEMPORAL_AVAILABLE:\n    print(\"  - temporal_comparison_v3.png\")\nprint(\"=\"*70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Download all files\nfrom google.colab import files\n\nprint(\"Downloading files...\")\nfiles.download('arxiv_ml_results_v3.json')\nfiles.download('shap_academic_v3.png')\nfiles.download('model_comparison_v3.png')\nfiles.download('analysis_types_v3.png')\n\nif TEMPORAL_AVAILABLE:\n    files.download('temporal_comparison_v3.png')\n\nprint(\"\\nAll files downloaded!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}